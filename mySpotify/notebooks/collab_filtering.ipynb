{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9efd9c06-1771-4483-819a-05451d3c5c07",
   "metadata": {},
   "source": [
    "# Demystifying Recommendations: Their Nature and Purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945594bf-cb0d-4a4c-a839-f9f24b71da9c",
   "metadata": {},
   "source": [
    "Ever wondered why Netflix suggests certain movies or TV shows for your next binge? Or why does your Spotify playlist seem like it's reading your mind? Magic? Not quite. In both instances, the power of machine learning-based recommendation models is at work.\n",
    "\n",
    "These models discern patterns in your viewing or listening habits and then find movies, TV shows, or songs that bear similarity to your tastes. Following this, they curate a list of recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80767e95-bbd7-4c2a-abfb-cb1dcf754207",
   "metadata": {},
   "source": [
    "## The Rationale Behind Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9560c1-447f-49f4-9ff7-f78490efca7f",
   "metadata": {},
   "source": [
    "The primary role of a recommendation system is to guide users in uncovering captivating content within an extensive collection. Consider platforms like Netflix, with its vast repertoire of movies and TV shows, or Spotify, a platform that hosts millions of tracks. These platforms are updated regularly with new content. So, how can users discover fresh and intriguing content amidst this sea of choices?\n",
    "\n",
    "While search functions serve as a means to directly access content, they rely on the user knowing what they are looking for. This is where a recommendation engine stands out. It has the ability to surface items that the user might not have thought to seek out independently, thus enriching their overall experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef97cb6b-edb9-4cb8-8bdd-e736214fac3a",
   "metadata": {},
   "source": [
    "## Unraveling the Lingo of Recommendation Systems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed5a834-6d7e-4081-97c1-ae9ca2f5d6af",
   "metadata": {},
   "source": [
    "In our journey to understand recommendation systems, it's vital to get acquainted with some key terms and concepts. Here's your cheat sheet:\n",
    "\n",
    "1. **Items (or Documents)**: These are the entities that the system is recommending. If we're talking about Netflix, items would be the movies and TV shows. For Spotify, the items would be songs or playlists.\n",
    "\n",
    "2. **Query (or Context)**: This represents the information the system uses to generate recommendations. Queries could include a mix of the following:\n",
    "\n",
    "   - User information or the user's ID.\n",
    "   - Items that the user has previously interacted with.  \n",
    "\n",
    "4. **Embedding**: This is a transformation from a discrete set (like the set of queries or items to recommend) to a vector space, known as the embedding space. The effectiveness of many recommendation systems hinges on learning suitable embedding representations for queries and items.\n",
    "\n",
    "5. **Recommendation Systems**: These are algorithms designed to suggest products, services, or information to users based on analysis of data.\n",
    "\n",
    "6. **Collaborative Filtering**: This technique uses other users' habits to recommend products to a user. The underlying assumption is that if users A and B have similar tastes on one issue, they are likely to have similar opinions on others.\n",
    "\n",
    "7. **Content-Based Filtering**: This technique recommends items by comparing the content of the items to a user's profile. Each item is represented by a set of descriptors, such as the words in a document.\n",
    "\n",
    "8. **User-Item Matrix**: A matrix used in collaborative filtering where each row represents a user, and each column represents an item. The entries can be explicit data (like ratings) or implicit data (like number of purchases).\n",
    "\n",
    "9. **Cold Start Problem**: This issue arises when the system cannot draw any inferences for users or items about which it has not yet gathered sufficient information. This is a common problem in recommendation systems, particularly for new users (user cold-start) or new items (item cold-start).\n",
    "\n",
    "10. **Implicit and Explicit Feedback**: Explicit feedback is input directly provided by the user (like ratings), while implicit feedback is gathered from user actions (like browsing history)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5de63a-9f95-42da-83c9-13fc87d36772",
   "metadata": {},
   "source": [
    "## Recommendation systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6d9288-ce42-4254-8a23-2705ba95a0c0",
   "metadata": {},
   "source": [
    "Recommendation systems are your secret sauce for a personalized user experience, nudging users towards content they'll adore. They're the backbone of e-commerce, streaming, and social platforms, guiding choices by harnessing the power of data.\n",
    "\n",
    "Their magic lies in learning from past user behaviors. Your behavior, fused with insights from others' interactions, shapes what you see next.\n",
    "\n",
    "Staggering stats attest to their influence: 40% of Google Play app installs, 60% of YouTube watch time, 35% of Amazon purchases, and 75% of Netflix watches are recommendation-driven.\n",
    "\n",
    "At the heart of it, these systems pivot around two key strategies: Content-Based Filtering, which aligns recommendations with a user's known preferences, and Collaborative Filtering, that draws on collective user insights to predict individual interests. Each strategy, with its unique strengths, plays a pivotal role in the art of making spot-on suggestions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dda515d-3516-44b3-8133-e0d2f6383fc5",
   "metadata": {},
   "source": [
    "| Type | Definition | Example |\n",
    "|---|---|---|\n",
    "| Content-Based Filtering | Uses similarity between items to recommend items similar to what the user likes. | If user A watches two cute cat videos, then the system can recommend cute animal videos to that user. |\n",
    "| Collaborative Filtering | Uses similarities between queries and items simultaneously to provide recommendations. | If user A is similar to user B, and user B likes video 1, then the system can recommend video 1 to user A (even if user A hasn’t seen any videos similar to video 1). |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef9c188-ee84-47cc-9ce2-f38bb8337749",
   "metadata": {},
   "source": [
    "### Content-based Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cf21a6-017b-4a84-82f6-9e72755838b6",
   "metadata": {},
   "source": [
    "Content-based filtering uses item characteristics to suggest items that are similar to what a user likes, based on their prior actions or explicit feedback.\n",
    "\n",
    "To illustrate content-based filtering, let's craft some features for Spotify. Consider a feature matrix where each row signifies a song, and each column represents a feature. These features could include genres (like pop, rock, or jazz), the artist, the album, and many others. For the sake of simplicity, let's assume this feature matrix is binary: a non-zero value means the song possesses that feature.\n",
    "\n",
    "We also represent the user in the same feature space. Some user-related features could be explicitly provided by the user. For instance, a user might select \"Rock music\" in their profile. Other features can be implicit, based on the songs they've previously listened to. For example, if the user has frequently played songs by the artist 'Queen'.\n",
    "\n",
    "The goal of the model is to recommend songs that would resonate with this user. To do this, you'd first choose a similarity metric (like the dot product). Then, you'd set up the system to score each candidate song according to this similarity metric. It's crucial to note that the recommendations are user-specific, as the model hasn't used any information about other users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bbd7d9-4922-4a08-b501-f409f46a4da6",
   "metadata": {},
   "source": [
    "Consider the following songs:\n",
    "\n",
    "- \"Bohemian Rhapsody\" by Queen\n",
    "- \"Imagine\" by John Lennon\n",
    "- \"Sweet Child o' Mine\" by Guns N' Roses\n",
    "- \"Stairway to Heaven\" by Led Zeppelin\n",
    "- \"Bad Guy\" by Billie Eilish\n",
    "\n",
    "Let's also consider three music genres: Rock, Pop, and Classic Rock. We can create a binary feature matrix to represent these songs and their genres, where '1' represents the presence of a feature and '0' represents the absence of it.\n",
    "\n",
    "| Song                     | Artist          | Rock | Pop | Classic Rock |\n",
    "|--------------------------|-----------------|------|-----|--------------|\n",
    "| Bohemian Rhapsody        | Queen           | 1    | 0   | 1            |\n",
    "| Imagine                  | John Lennon     | 0    | 1   | 0            |\n",
    "| Sweet Child o' Mine      | Guns N' Roses   | 1    | 0   | 1            |\n",
    "| Stairway to Heaven       | Led Zeppelin    | 1    | 0   | 1            |\n",
    "| Bad Guy                  | Billie Eilish   | 0    | 1   | 0            |\n",
    "\n",
    "Now, consider a user who has a preference for Rock and Classic Rock and has recently listened to songs by Queen and Led Zeppelin. We can represent this user's profile in the same feature space:\n",
    "\n",
    "| User Profile | Queen | John Lennon | Guns N' Roses | Led Zeppelin | Billie Eilish | Rock | Pop | Classic Rock |\n",
    "|--------------|-------|-------------|---------------|--------------|---------------|------|-----|--------------|\n",
    "| User 1       | 1     | 0           | 0             | 1            | 0             | 1    | 0   | 1            |\n",
    "\n",
    "Using content-based filtering, we would calculate the similarity between the user profile and each song in the feature matrix. This could be done with the cosine similarity, dot product, or any other similarity measure.\n",
    "\n",
    "For simplicity, let's use the dot product. The dot product between two vectors is the sum of the products of their corresponding entries. For binary vectors, it effectively counts the number of features they have in common. A higher dot product means more common features, and hence a higher similarity.\n",
    "\n",
    "The dot product between the user profile and each song will be as follows:\n",
    "\n",
    "- User 1 and \"Bohemian Rhapsody\": 4 (Queen, Rock, Classic Rock)\n",
    "- User 1 and \"Imagine\": 0\n",
    "- User 1 and \"Sweet Child o' Mine\": 2 (Rock, Classic Rock)\n",
    "- User 1 and \"Stairway to Heaven\": 4 (Led Zeppelin, Rock, Classic Rock)\n",
    "- User 1 and \"Bad Guy\": 0\n",
    "\n",
    "So, the system would recommend \"Bohemian Rhapsody\" and \"Stairway to Heaven\" to the user, as these songs have the highest similarity scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f181d323-c6b7-4678-b656-110f3965f5d6",
   "metadata": {},
   "source": [
    "### Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a23b78-6843-4ef2-8e50-823fc0d2c694",
   "metadata": {},
   "source": [
    "Think about a situation where you have a bunch of users and a bunch of items, and you're trying to figure out which items would most likely be of interest to which users. You see this problem everywhere, from recommending movies on Netflix, highlighting relevant content on a homepage, deciding what posts to show in a social media feed, and beyond. There's a nifty solution to this problem that's called collaborative filtering.\n",
    "\n",
    "Here's how collaborative filtering works: it looks at what items the current user has interacted with or liked, finds other users who have interacted with or liked similar items, and then recommends other items that those users have interacted with or liked.\n",
    "\n",
    "To give you a concrete example, let's say you've been watching a ton of sci-fi action movies from the 70s on Netflix. Now, Netflix might not have these specific details about the films you watched, but it can see that other people who watched the same movies you did also tend to watch other sci-fi action films from the 70s. The interesting part is that to use this approach, we don't really need to know anything about the movies themselves, except for who likes to watch them.\n",
    "\n",
    "Collaborative filtering is actually a way to solve a broader class of problems that doesn't necessarily involve users and products. In fact, we usually talk about items instead of products in the context of collaborative filtering. These items could be anything from links that people click on, to diagnoses selected for patients, and more.\n",
    "\n",
    "Now, the cornerstone of this whole approach is something called latent factors. In our Netflix example, we started with the assumption that you're into old, action-packed sci-fi movies. But you never actually told Netflix that you like these types of movies. And Netflix didn't have to add columns to its movie database saying which films are of these types. But, there must be some latent (or hidden) concept of sci-fi, action, and film age, and these concepts are likely relevant to the movie watching choices of at least some people."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89f371d-d7c7-4499-88e8-12c72fe9ffa1",
   "metadata": {},
   "source": [
    "Consider the following movies:\n",
    "\n",
    "- \"The Last Skywalker\"\n",
    "- \"Casablanca\"\n",
    "- \"Avengers: Endgame\"\n",
    "- \"The Godfather\"\n",
    "- \"Toy Story 4\"\n",
    "\n",
    "Let's also consider three movie categories: Science Fiction (Sci-Fi), Action, and Age (where a positive value represents an old movie and a negative value represents a new movie). We can create a feature matrix to represent these movies and their categories as follows:\n",
    "\n",
    "| Movie                 | Sci-Fi | Action | Age |\n",
    "|-----------------------|--------|--------|-----|\n",
    "| The Last Skywalker    | 0.98   | 0.9    | -0.9|\n",
    "| Casablanca            | -0.99  | -0.3   | 0.8 |\n",
    "| Avengers: Endgame     | 0.9    | 0.95   | -0.85|\n",
    "| The Godfather         | -0.8   | -0.4   | 0.9 |\n",
    "| Toy Story 4           | 0.85   | 0.8    | -0.75|\n",
    "\n",
    "Now, consider a user who enjoys modern sci-fi action movies. We can represent this user's profile in the same feature space:\n",
    "\n",
    "| User Profile | Sci-Fi | Action | Age |\n",
    "|--------------|--------|--------|-----|\n",
    "| User 1       | 0.9    | 0.8    | -0.6|\n",
    "\n",
    "Using the dot product calculation for each movie:\n",
    "\n",
    "- User 1 and \"The Last Skywalker\": $(0.9*0.98 + 0.8*0.9 + (-0.6)*(-0.9))$\n",
    "- User 1 and \"Casablanca\": $(0.9*(-0.99) + 0.8*(-0.3) + (-0.6)*0.8)$\n",
    "- User 1 and \"Avengers: Endgame\": $(0.9*0.9 + 0.8*0.95 + (-0.6)*(-0.85))$\n",
    "- User 1 and \"The Godfather\": $(0.9*(-0.8) + 0.8*(-0.4) + (-0.6)*0.9)$\n",
    "- User 1 and \"Toy Story 4\": $(0.9*0.85 + 0.8*0.8 + (-0.6)*(-0.75))$\n",
    "\n",
    "These calculations will give you the match between the user's preferences and each movie. The higher the score, the better the match, and hence the more likely the user is to enjoy the movie. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f80198-5dc1-4b48-bbc9-67049b225cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the feature vectors for the movies\n",
    "last_skywalker = np.array([0.98, 0.9, -0.9])\n",
    "casablanca = np.array([-0.99, -0.3, 0.8])\n",
    "avengers_endgame = np.array([0.9, 0.95, -0.85])\n",
    "the_godfather = np.array([-0.8, -0.4, 0.9])\n",
    "toy_story_4 = np.array([0.85, 0.8, -0.75])\n",
    "\n",
    "# Define the user profile\n",
    "user1 = np.array([0.9, 0.8, -0.6])\n",
    "\n",
    "# Calculate the dot product between the user profile and each movie\n",
    "score_last_skywalker = np.dot(user1, last_skywalker)\n",
    "score_casablanca = np.dot(user1, casablanca)\n",
    "score_avengers_endgame = np.dot(user1, avengers_endgame)\n",
    "score_the_godfather = np.dot(user1, the_godfather)\n",
    "score_toy_story_4 = np.dot(user1, toy_story_4)\n",
    "\n",
    "score_last_skywalker, score_casablanca, score_avengers_endgame, score_the_godfather, score_toy_story_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea74e40f-7241-4860-a707-652ad48a60a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a dictionary with the movie names and their corresponding scores\n",
    "data = {\n",
    "    'Movie': ['The Last Skywalker', 'Casablanca', 'Avengers: Endgame', 'The Godfather', 'Toy Story 4'],\n",
    "    'Score': [score_last_skywalker, score_casablanca, score_avengers_endgame, score_the_godfather, score_toy_story_4]\n",
    "}\n",
    "\n",
    "# Create a pandas DataFrame from the dictionary\n",
    "df_scores = pd.DataFrame(data)\n",
    "\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c709e6-87b5-423e-ab72-ee42bf8d544a",
   "metadata": {},
   "source": [
    "The positive scores indicate a good match between the user's preferences and the movie, while the negative scores indicate a mismatch. The higher the score, the better the match.\n",
    "\n",
    "So, according to these scores, the user is most likely to enjoy \"The Last Skywalker\", followed by \"Avengers: Endgame\" and \"Toy Story 4\". The user is least likely to enjoy \"Casablanca\" and \"The Godfather\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695f1f82-54e1-4576-b9d7-ad5e5742ca5d",
   "metadata": {},
   "source": [
    "In practice, we often don't know what the latent factors are or how to score them for each user and item. We also don't know how many latent factors there should be. However, we can learn them from the data.\n",
    "\n",
    "The idea is to start with random values for the user and item factors, and then iteratively adjust these values in a way that minimizes the difference between the predicted and actual ratings. This process is known as matrix factorization, and it can be accomplished using techniques like singular value decomposition (SVD) or algorithms like stochastic gradient descent (SGD).\n",
    "\n",
    "In this approach, each user and each item is represented by a vector of latent factors. The dot product of a user vector and an item vector gives the predicted rating for that item by that user. The goal of the learning process is to find the values for these vectors that give the best predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a26bd2-9ff9-4d8d-b954-947845e047ab",
   "metadata": {},
   "source": [
    "### Diving Into the Embedding Space (Latent-Factors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaeae1e-2447-4bba-8420-08126ca799f9",
   "metadata": {},
   "source": [
    "Both content-based and collaborative filtering techniques translate each item and each query (or context) into an embedding vector in a shared embedding space, $E = R^d$. Typically, this space is low-dimensional (meaning, $d$ is significantly smaller than the corpus size) and encapsulates some latent structure of the item or query set. Items bearing similarity, like movies typically watched by the same user, find themselves in close proximity in the embedding space. This idea of \"closeness\" is determined by a similarity measure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964331c6-b232-4da5-8d4e-0b2d8e42b238",
   "metadata": {},
   "source": [
    "#### Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc2de96-6293-4ec5-be75-68a635204134",
   "metadata": {},
   "source": [
    "Matrix factorization is a simple embedding model. Given the feedback matrix $A \\in \\mathbb{R}^{n \\times m}$ where $m$ is the number of users (or queries) and $n$ is the number of items, the model learns:\n",
    "\n",
    "- A user embedding matrix $U \\in \\mathbb{R}^{m \\times d}$ where row $i$ is the embedding for user $i$.\n",
    "- An item embedding matrix $V \\in \\mathbb{R}^{n \\times d}$ where row $j$ is the embedding for item $j$.\n",
    "\n",
    "It learns a dense representation (embedding) for both users and items in a shared low-dimensional space. This space is of dimension $d$, which is typically much smaller than $m$ (the number of users) or $n$ (the number of items). \n",
    "\n",
    "The key idea is that the interaction between a user and an item can be modeled as the dot product of their respective embeddings:\n",
    "\n",
    "\n",
    "$$A_{ij} \\approx U_i \\cdot V_j^T$$\n",
    "\n",
    "\n",
    "where $A_{ij}$ is the element in the $i$-th row and $j$-th column of the feedback matrix $A$, $U_i$ is the $i$-th row of the user embedding matrix $U$, and $V_j$ is the $j$-th row of the item embedding matrix $V$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3363d4e1-cbe6-46dc-98b4-b0e049c73a60",
   "metadata": {},
   "source": [
    "### Learning the Latent Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daee836a-70b5-4cfb-8afa-0c4a8b8de7fe",
   "metadata": {},
   "source": [
    "Certainly, here are the steps in LaTeX:\n",
    "\n",
    "**1.** The first step is to randomly initialize some parameters known as latent factors for each user and movie. These latent factors can be represented as vectors $U_i$ and $V_j$ for user $i$ and movie $j$, respectively. The entries of these vectors are displayed next to the users and movies in our crosstab, and the results of multiplying each combination of these entries (dot products) are filled in the middle.\n",
    "\n",
    "**2.** The second step is to calculate our predictions by taking the dot product of each movie's latent factors with each user's latent factors. If we let $P_{ij}$ represent the predicted rating for user $i$ and movie $j$, we can calculate all predictions as follows:\n",
    "\n",
    "$$\n",
    "P_{ij} = U_i \\cdot V_j^T\n",
    "$$\n",
    "\n",
    "The product will be high if the user's preferences and the movie's characteristics align, and low if they don't.\n",
    "\n",
    "**3.** The third step is to calculate our loss using a loss function. In this case, we're using mean squared error (MSE), which measures the average of the squares of the differences between the predicted and actual ratings. This can be represented as follows:\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{N} \\sum_{(i,j) \\in \\text{observed}} (A_{ij} - P_{ij})^2\n",
    "$$\n",
    "\n",
    "where $N$ is the number of observed ratings, $A_{ij}$ is the actual rating for user $i$ and movie $j$, and $P_{ij}$ is the predicted rating.\n",
    "\n",
    "**4.** With these in place, we optimize our parameters (latent factors) using stochastic gradient descent (SGD) to minimize the loss. This process calculates the gradient of the loss with respect to the latent factors, and then adjusts the latent factors by taking a step in the direction of steepest descent. This process, repeated many times, reduces the loss and thus improves the quality of our recommendations. The update rule for SGD can be represented as follows:\n",
    "\n",
    "$$\n",
    "U_i = U_i - \\alpha \\frac{\\partial L}{\\partial U_i}, \\quad V_j = V_j - \\alpha \\frac{\\partial L}{\\partial V_j}\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is the learning rate, and $\\frac{\\partial L}{\\partial U_i}$ and $\\frac{\\partial L}{\\partial V_j}$ are the gradients of the loss with respect to $U_i$ and $V_j$, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f279256-cfd3-44b4-bc25-a49fb0abc06c",
   "metadata": {},
   "source": [
    "# Fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5b0a37-42d4-4b6c-b6b4-ad59db8daaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.collab import *\n",
    "from fastai.tabular.all import *\n",
    "path = untar_data(URLs.ML_100k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784f1a80-02b1-4f0d-8e2d-fcbf63f53499",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv(path/'u.data', delimiter='\\t', header=None,\n",
    "                      names=['user','movie','rating','timestamp'])\n",
    "ratings.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0c4afe-023c-4609-85b5-78f4009e88b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.drop(['timestamp'], axis=1, inplace=True)\n",
    "ratings.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12038558-ea56-45c6-80bf-5f2bbb828732",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv(path/'u.item',  delimiter='|', encoding='latin-1',\n",
    "                     usecols=(0,1), names=('movie','title'), header=None)\n",
    "movies.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a1e9e2-8b15-4847-b1ee-1d668d84a2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_skywalker = np.array([0.98,0.9,-0.9])\n",
    "user1 = np.array([0.9,0.8,-0.6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1149bef-e57f-4239-b1fb-fd7381b1c085",
   "metadata": {},
   "outputs": [],
   "source": [
    "(last_skywalker*user1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2191181-5932-43c9-94f8-0cc33d2d0a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "casablanca = np.array([-0.99,-0.3,0.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82549346-a423-448b-9156-7b4b188f1a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "(casablanca*user1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836b1dd8-e6a7-4bf0-9d8b-6816215cb422",
   "metadata": {},
   "source": [
    "## 1. Learning the Latent Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496ec15e-a3de-483a-91a5-7f44c6751872",
   "metadata": {},
   "source": [
    "**1.** The first step involves randomly initializing some parameters known as latent factors for each user and movie. These values are displayed next to the users and movies in our table (or crosstab), and the results of multiplying each combination of these elements (dot products) are filled in the middle.\n",
    "\n",
    "**2.** The second step is to calculate our predictions by taking the dot product of each movie with each user. The product will be high if the user's preferences and the movie's characteristics match, and low if they don't.\n",
    "\n",
    "**3.** The third step is to calculate our loss using a loss function, in this case, we're using mean squared error, which represents the accuracy of a prediction.\n",
    "\n",
    "**4.** With these in place, we optimize our parameters (latent factors) using stochastic gradient descent to minimize the loss. This process calculates the match between each movie and each user, compares it to the actual rating, calculates the derivative, and adjusts the weights using the learning rate. This process, repeated multiple times, improves the loss and thus the quality of recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5219bccc-cc51-453b-a545-c1cb9798547c",
   "metadata": {},
   "source": [
    "5. In machine learning, we often use a technique called 'one-hot encoding' to represent data. This technique, however, can consume a lot of memory and time. So, instead, we use an 'embedding' approach. \n",
    "\n",
    "6. An 'embedding' is a computational shortcut to one-hot encoding. It uses an integer to index directly into a vector (a list of numbers). This indexing approach behaves as if it had done a matrix multiplication with a one-hot-encoded vector. The vector we index into directly is called the 'embedding matrix'.\n",
    "\n",
    "7. In computer vision, each pixel in an image is represented by three numbers: the RGB values. This is a straightforward way to characterize a pixel.\n",
    "\n",
    "8. When dealing with complex data, like a user's movie preference, characterizing isn't that simple. A user's preference can be influenced by factors like genre, dialogue, action content, or specific actors. \n",
    "\n",
    "9. Instead of manually assigning numbers to characterize these complex factors, we let our machine learning model learn them by analyzing user-movie interactions.\n",
    "\n",
    "10. To do this, we assign each user and movie a random vector of a certain length (5 in this case), and these become learnable parameters. The model then adjusts these parameters as it learns from the data.\n",
    "\n",
    "11. Initially, these randomly chosen numbers don't have any meaning, but by the end of the training, they do. The model can pick up on important features, like differentiating between blockbuster and independent cinema, action movies from romance, and so on.\n",
    "\n",
    "12. With these concepts understood, we are ready to build our model from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dc6a1d-f22f-4df2-8442-0929c052aa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings.merge(movies)\n",
    "ratings.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f07dc3-4a7e-41b7-8535-c32153f655ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = CollabDataLoaders.from_df(ratings, item_name='title', bs=64)\n",
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b2d7f8-f761-46e0-8c28-96cef063c52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.classes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f740d316-de95-4334-8451-556de316b95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.classes['user']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e7659c-1587-483c-96aa-30bb4632b118",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.classes['title'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57e7e5a-6522-472e-a11d-89d8dba2b11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users  = len(dls.classes['user'])\n",
    "n_movies = len(dls.classes['title'])\n",
    "n_factors = 5\n",
    "\n",
    "user_factors = torch.randn(n_users, n_factors)\n",
    "movie_factors = torch.randn(n_movies, n_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca47353-755b-42fb-9f03-19e31ce3ff7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users, n_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e9fb93-dcbd-406b-9cb9-e3e6c70e8bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_factors.shape, movie_factors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ce05a3-7f83-4f9e-9d9a-a6d98e3c3ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_3 = one_hot(3, n_users).float()\n",
    "one_hot_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3206fe62-a283-42d2-aa59-9c57e078b67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adca1011-1328-498c-9929-9d45aaebce18",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_factors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0e20a8-ca7e-45d1-9bec-9ed8b99d188a",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_factors.t() @ one_hot_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15a0b55-fd41-4178-ba20-d10bf9583945",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_factors[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d95c4d-181e-4553-b4a2-0b91af32a01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_ = np.eye(5, n_users)\n",
    "one_hot_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e222d9-6fcb-4913-8e0a-b798c2d7c923",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_factors.t() @ one_hot_.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69365889-4deb-436d-93b8-3b50af0f57ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_factors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67ebff7-f0d5-481e-87f9-9cc041c18559",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd685453-0a32-489a-bbda-b28ca7a8df6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProduct(Module):\n",
    "    def __init__(self, n_users, n_movies, n_factors):\n",
    "        self.user_factors = Embedding(n_users, n_factors)\n",
    "        self.movie_factors = Embedding(n_movies, n_factors)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        users = self.user_factors(x[:,0])\n",
    "        movies = self.movie_factors(x[:,1])\n",
    "        return (users * movies).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e11fc30-5dea-4c20-a7f4-51ff27aa711b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = dls.one_batch()\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70573ec4-cf40-4e88-b637-24fa56d64d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2dbe02-7ad1-4ee5-9c4b-2dc2dac6af72",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:, 0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1df9a89-3e24-464f-b5b1-f23c12702786",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:, 1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15922371-bd88-42fd-95f3-9c92d0974539",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x[:, 0][:10] * x[:, 1][:10]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3925b56-1a99-425c-87ba-91bccd60a1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DotProduct(n_users, n_movies, 50)\n",
    "learn = Learner(dls, model, loss_func=MSELossFlat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5a9189-dcdc-463d-a342-d61237edad5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(5, 5e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30041990-e401-43e5-86a7-cccba579d11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProduct(Module):\n",
    "    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n",
    "        self.user_factors = Embedding(n_users, n_factors)\n",
    "        self.movie_factors = Embedding(n_movies, n_factors)\n",
    "        self.y_range = y_range\n",
    "        \n",
    "    def forward(self, x):\n",
    "        users = self.user_factors(x[:,0])\n",
    "        movies = self.movie_factors(x[:,1])\n",
    "        return sigmoid_range((users * movies).sum(dim=1), *self.y_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6290cd-ca64-4446-b165-0f29a0fc350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DotProduct(n_users, n_movies, 50)\n",
    "learn = Learner(dls, model, loss_func=MSELossFlat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727c32fb-2ed3-4f21-b7b1-c577f3230f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(5, 5e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667acab3-87fc-4ec8-aa3f-a364886982ec",
   "metadata": {},
   "source": [
    "The text is discussing the limitations of a machine learning model used for predicting movie ratings, specifically a model that relies solely on the dot product of user and movie latent factors (these are essentially characteristics or features that the model has learned).\n",
    "\n",
    "According to the text, the model is currently unable to account for the inherent bias some users might have towards being more positive or negative in their ratings. Similarly, the model cannot account for the inherent quality of movies - that some movies are just generally liked or disliked, regardless of their specific characteristics.\n",
    "\n",
    "For instance, if a movie is characterized by the model as very sci-fi, very action-oriented, and very new, the model doesn't have a way to capture whether the movie is generally well-liked or not. These characteristics tell us about the movie's genre and style, but not its overall quality or general reception.\n",
    "\n",
    "The text suggests adding biases as a solution to this problem. A bias is a term in machine learning that allows us to shift our predictions by a constant value. For this case, we can have a bias for each user (representing their general positivity or negativity) and for each movie (representing its general quality). By adding these biases to our predictions, the model would be able to make more accurate predictions.\n",
    "\n",
    "So, the model architecture needs to be adjusted to include these biases for each user and movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b696b6e5-e01d-4b91-99f4-9ea8d62a6821",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductBias(Module):\n",
    "    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n",
    "        self.user_factors = Embedding(n_users, n_factors)\n",
    "        self.user_bias = Embedding(n_users, 1)\n",
    "        self.movie_factors = Embedding(n_movies, n_factors)\n",
    "        self.movie_bias = Embedding(n_movies, 1)\n",
    "        self.y_range = y_range\n",
    "        \n",
    "    def forward(self, x):\n",
    "        users = self.user_factors(x[:,0])\n",
    "        movies = self.movie_factors(x[:,1])\n",
    "        res = (users * movies).sum(dim=1, keepdim=True)\n",
    "        res += self.user_bias(x[:,0]) + self.movie_bias(x[:,1])\n",
    "        return sigmoid_range(res, *self.y_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41423931-7e93-42c4-a00b-67c5d1213c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DotProduct(n_users, n_movies, 50)\n",
    "learn = Learner(dls, model, loss_func=MSELossFlat())\n",
    "learn.fit_one_cycle(5, 5e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b005976-6bb2-4406-9c46-b5be383747b9",
   "metadata": {},
   "source": [
    "### Using weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5030b58a-e2d9-4e81-bd50-3e51f498bcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DotProductBias(n_users, n_movies, n_factors=50)\n",
    "learn = Learner(dls, model, loss_func=MSELossFlat())\n",
    "learn.fit_one_cycle(5, 4e-3, wd=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cc15f7-0ecd-4546-83d4-571ede067abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78632b6-1b36-43b2-b99f-8028978a8d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.user_factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb81ba0-e5b6-4437-92e4-f99a16db74db",
   "metadata": {},
   "source": [
    "## Creating Our Own Embedding Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a025de78-0b41-4441-804b-142f345b2ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_params(size):\n",
    "    return nn.Parameter(torch.zeros(*size).normal_(0, 0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed20211-a3a1-4a88-9862-fd4b4cf90ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductBias(Module):\n",
    "    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n",
    "        self.user_factors = create_params([n_users, n_factors])\n",
    "        self.user_bias = create_params([n_users])\n",
    "        self.movie_factors = create_params([n_movies, n_factors])\n",
    "        self.movie_bias = create_params([n_movies])\n",
    "        self.y_range = y_range\n",
    "        \n",
    "    def forward(self, x):\n",
    "        users = self.user_factors[x[:,0]]\n",
    "        movies = self.movie_factors[x[:,1]]\n",
    "        res = (users*movies).sum(dim=1)\n",
    "        res += self.user_bias[x[:,0]] + self.movie_bias[x[:,1]]\n",
    "        return sigmoid_range(res, *self.y_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b57f70b-088f-4470-90fe-fc932d64fb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DotProductBias(n_users, n_movies, 50)\n",
    "learn = Learner(dls, model, loss_func=MSELossFlat())\n",
    "learn.fit_one_cycle(5, 5e-3, wd=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58182025-9dc9-49e5-99e4-cceb627f72c8",
   "metadata": {},
   "source": [
    "##  Interpreting Embeddings and Biases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366e782c-85cc-40ad-8c9a-cd71604219fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_bias = learn.model.movie_bias.squeeze()\n",
    "idxs = movie_bias.argsort()[:5]\n",
    "[dls.classes['title'][i] for i in idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9a1f26-1912-4660-94fa-01bd61af63a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_bias[idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916ec128-185a-4c00-88f4-09736e66c3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = movie_bias.argsort(descending=True)[:5]\n",
    "[dls.classes['title'][i] for i in idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9585b83-feaa-4a70-9444-f58b3706ba61",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_bias[idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c378bd21-9679-4cb1-bf4b-094b5d39dd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide_input\n",
    "#id img_pca_movie\n",
    "#caption Representation of movies based on two strongest PCA components\n",
    "#alt Representation of movies based on two strongest PCA components\n",
    "g = ratings.groupby('title')['rating'].count()\n",
    "top_movies = g.sort_values(ascending=False).index.values[:1000]\n",
    "top_idxs = tensor([learn.dls.classes['title'].o2i[m] for m in top_movies])\n",
    "movie_w = learn.model.movie_factors[top_idxs].cpu().detach()\n",
    "movie_pca = movie_w.pca(3)\n",
    "fac0,fac1,fac2 = movie_pca.t()\n",
    "idxs = list(range(50))\n",
    "X = fac0[idxs]\n",
    "Y = fac2[idxs]\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.scatter(X, Y)\n",
    "for i, x, y in zip(top_movies[idxs], X, Y):\n",
    "    plt.text(x,y,i, color=np.random.rand(3)*0.7, fontsize=11)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf3497c-8ed4-4f59-8195-e297bc0075f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = collab_learner(dls, n_factors=50, y_range=(0, 5.5))\n",
    "learn.fit_one_cycle(5, 5e-3, wd=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdc6cb0-acdc-4a6c-a25a-946924ef6948",
   "metadata": {},
   "source": [
    "### Embedding Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7b2728-671f-47fb-a8e9-8f4162c7a352",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_factors = learn.movie_factors\n",
    "idx = dls.classes['title'].o2i['Silence of the Lambs, The (1991)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df270bb1-99ca-4ed5-8145-b2970f9aaa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_factors.shape, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c9baf2-0ff5-4840-a84f-3c8aac07fcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = nn.CosineSimilarity(dim=1)(movie_factors, movie_factors[idx][None])\n",
    "idx = distances.argsort(descending=True)[1]\n",
    "dls.classes['title'][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad61a75-5d33-421e-9fd8-c20f28806f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_factors = learn.model.i_weight.weight\n",
    "idx = dls.classes['title'].o2i['Silence of the Lambs, The (1991)']\n",
    "distances = nn.CosineSimilarity(dim=1)(movie_factors, movie_factors[idx][None])\n",
    "idx = distances.argsort(descending=True)[:5]\n",
    "dls.classes['title'][idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a0e69a-05be-4754-99b2-6f6df66dcf13",
   "metadata": {},
   "source": [
    "### Bootstrapping a Collaborative Filtering Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecef494-4446-4394-b65c-199cdb85c5ff",
   "metadata": {},
   "source": [
    "The biggest challenge with using collaborative filtering models in practice is the bootstrapping problem. The most extreme version of this problem is when you have no users, and therefore no history to learn from. What products do you recommend to your very first user?\n",
    "\n",
    "But even if you are a well-established company with a long history of user transactions, you still have the question: what do you do when a new user signs up? And indeed, what do you do when you add a new product to your portfolio? There is no magic solution to this problem, and really the solutions that we suggest are just variations of use your common sense. You could assign new users the mean of all of the embedding vectors of your other users, but this has the problem that that particular combination of latent factors may be not at all common (for instance, the average for the science-fiction factor may be high, and the average for the action factor may be low, but it is not that common to find people who like science-fiction without action). Better would probably be to pick some particular user to represent average taste.\n",
    "\n",
    "Better still is to use a tabular model based on user meta data to construct your initial embedding vector. When a user signs up, think about what questions you could ask them that could help you to understand their tastes. Then you can create a model where the dependent variable is a user’s embedding vector, and the independent variables are the results of the questions that you ask them, along with their signup metadata. We will see in the next section how to create these kinds of tabular models. (You may have noticed that when you sign up for services such as Pandora and Netflix, they tend to ask you a few questions about what genres of movie or music you like; this is how they come up with your initial collaborative filtering recommendations.)\n",
    "\n",
    "One thing to be careful of is that a small number of extremely enthusiastic users may end up effectively setting the recommendations for your whole user base. This is a very common problem, for instance, in movie recommendation systems. People that watch anime tend to watch a whole lot of it, and don’t watch very much else, and spend a lot of time putting their ratings on websites. As a result, anime tends to be heavily overrepresented in a lot of best ever movies lists. In this particular case, it can be fairly obvious that you have a problem of representation bias, but if the bias is occurring in the latent factors then it may not be obvious at all.\n",
    "\n",
    "Such a problem can change the entire makeup of your user base, and the behavior of your system. This is particularly true because of positive feedback loops. If a small number of your users tend to set the direction of your recommendation system, then they are naturally going to end up attracting more people like them to your system. And that will, of course, amplify the original representation bias. This type of bias has a natural tendency to be amplified exponentially. You may have seen examples of company executives expressing surprise at how their online platforms rapidly deteriorated in such a way that they expressed values at odds with the values of the founders. In the presence of these kinds of feedback loops, it is easy to see how such a divergence can happen both quickly and in a way that is hidden until it is too late.\n",
    "\n",
    "In a self-reinforcing system like this, we should probably expect these kinds of feedback loops to be the norm, not the exception. Therefore, you should assume that you will see them, plan for that, and identify up front how you will deal with these issues. Try to think about all of the ways in which feedback loops may be represented in your system, and how you might be able to identify them in your data. In the end, this is coming back to our original advice about how to avoid disaster when rolling out any kind of machine learning system. It’s all about ensuring that there are humans in the loop; that there is careful monitoring, and a gradual and thoughtful rollout.\n",
    "\n",
    "Our dot product model works quite well, and it is the basis of many successful real-world recommendation systems. This approach to collaborative filtering is known as probabilistic matrix factorization (PMF). Another approach, which generally works similarly well given the same data, is deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03fac7f-f791-4f60-8fe0-58d337b12e4b",
   "metadata": {},
   "source": [
    "### Deep Learning for Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fb61c0-81c9-4489-b118-24d69761cf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = get_emb_sz(dls)\n",
    "embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea15f41-76a6-46ab-b9fe-703b23d85481",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollabNN(Module):\n",
    "    def __init__(self, user_sz, item_sz, y_range=(0,5.5), n_act=100):\n",
    "        self.user_factors = Embedding(*user_sz)\n",
    "        self.item_factors = Embedding(*item_sz)\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(user_sz[1]+item_sz[1], n_act),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_act, 1))\n",
    "        self.y_range = y_range\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # import pdb; pdb.set_trace()\n",
    "        embs = self.user_factors(x[:,0]),self.item_factors(x[:,1])\n",
    "        x = self.layers(torch.cat(embs, dim=1))\n",
    "        return sigmoid_range(x, *self.y_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90996417-eb3d-410d-a06a-da65e07a8d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CollabNN(*embs)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df391ae5-120b-4136-826b-604596bf1536",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, model, loss_func=MSELossFlat())\n",
    "learn.fit_one_cycle(5, 5e-3, wd=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f1e890-314a-412f-98ca-109f1c3eb998",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21863758-b652-4df2-b14c-acfc552283e3",
   "metadata": {},
   "source": [
    "# Colab: Build a Movie Recommendation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3447093-1e69-4a3f-ba36-76654b5c1fae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
